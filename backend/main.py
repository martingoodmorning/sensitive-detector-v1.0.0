from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware  # è§£å†³å‰ç«¯è·¨åŸŸé—®é¢˜
from fastapi.staticfiles import StaticFiles  # é™æ€æ–‡ä»¶æœåŠ¡
from fastapi.responses import FileResponse  # æ–‡ä»¶å“åº”
from pydantic import BaseModel  # æ ¡éªŒè¯·æ±‚å‚æ•°æ ¼å¼
from typing import List, Optional, Dict, Any
import docx  # è§£ædocxæ–‡æ¡£
import PyPDF2  # è§£æpdfæ–‡æ¡£
import docx2txt  # è§£ædocæ–‡æ¡£
import pytesseract  # OCRæ–‡å­—è¯†åˆ«
from PIL import Image  # å›¾åƒå¤„ç†
import os
import json
import requests
from io import BytesIO
import glob
from datetime import datetime
import time
import subprocess  # ç”¨äºè°ƒç”¨antiwordå·¥å…·
import tempfile  # ç”¨äºåˆ›å»ºä¸´æ—¶æ–‡ä»¶

# 1. åˆå§‹åŒ–FastAPIåº”ç”¨
app = FastAPI(title="æ•æ„Ÿè¯æ£€æµ‹", version="1.0", docs_url="/api/docs", redoc_url="/api/redoc")

# æ ¹è·¯å¾„é‡å®šå‘åˆ°å‰ç«¯é¡µé¢
@app.get("/", include_in_schema=False)
async def read_root():
    return FileResponse("/app/frontend/index.html")

# APIæ–‡æ¡£è·¯å¾„
@app.get("/docs", include_in_schema=False)
async def read_docs():
    return FileResponse("/app/frontend/index.html")

# æŒ‚è½½é™æ€æ–‡ä»¶ç›®å½•ï¼ˆå®¹å™¨å†…è·¯å¾„ï¼‰- å¿…é¡»åœ¨è·¯ç”±ä¹‹å
app.mount("/static", StaticFiles(directory="/app/frontend"), name="static")

# 2. è§£å†³è·¨åŸŸï¼ˆå‰ç«¯åœ¨Windowsæµè§ˆå™¨ï¼Œåç«¯åœ¨WSLå®¹å™¨ï¼Œå¿…é¡»å…è®¸è·¨åŸŸï¼‰
allow_origins_env = os.getenv("CORS_ALLOW_ORIGINS", "*")
allow_origins = [o.strip() for o in allow_origins_env.split(",")] if allow_origins_env else ["*"]
app.add_middleware(
    CORSMiddleware,
    allow_origins=allow_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ---------------------- æ•æ„Ÿè¯åº“ç®¡ç† ----------------------
class WordLibraryManager:
    """æ•æ„Ÿè¯åº“ç®¡ç†å™¨"""
    
    def __init__(self, base_path="/app/word_libraries"):
        self.base_path = base_path
        self.ensure_base_directory()
    
    def ensure_base_directory(self):
        """ç¡®ä¿åŸºç¡€ç›®å½•å­˜åœ¨"""
        if not os.path.exists(self.base_path):
            os.makedirs(self.base_path, exist_ok=True)
    
    def get_library_list(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰æ•æ„Ÿè¯åº“åˆ—è¡¨"""
        libraries = []
        
        # æ‰«ææ‰€æœ‰.txtæ–‡ä»¶
        pattern = os.path.join(self.base_path, "*.txt")
        for file_path in glob.glob(pattern):
            filename = os.path.basename(file_path)
            name = os.path.splitext(filename)[0]
            
            # è·å–æ–‡ä»¶ä¿¡æ¯
            stat = os.stat(file_path)
            word_count = self._count_words_in_file(file_path)
            
            libraries.append({
                "id": name,
                "name": name,
                "filename": filename,
                "path": file_path,
                "word_count": word_count,
                "created_time": datetime.fromtimestamp(stat.st_ctime).isoformat(),
                "modified_time": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                "size": stat.st_size
            })
        
        return sorted(libraries, key=lambda x: x["name"])
    
    def _count_words_in_file(self, file_path: str) -> int:
        """ç»Ÿè®¡æ–‡ä»¶ä¸­çš„æ•æ„Ÿè¯æ•°é‡"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                return len([line.strip() for line in f if line.strip()])
        except:
            return 0
    
    def create_library(self, name: str, words: List[str]) -> Dict[str, Any]:
        """åˆ›å»ºæ–°çš„æ•æ„Ÿè¯åº“"""
        filename = f"{name}.txt"
        file_path = os.path.join(self.base_path, filename)
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if os.path.exists(file_path):
            raise HTTPException(status_code=400, detail=f"æ•æ„Ÿè¯åº“ '{name}' å·²å­˜åœ¨")
        
        # å†™å…¥æ–‡ä»¶
        with open(file_path, "w", encoding="utf-8") as f:
            for word in words:
                f.write(word.strip() + "\n")
        
        return {
            "id": name,
            "name": name,
            "filename": filename,
            "path": file_path,
            "word_count": len(words),
            "created_time": datetime.now().isoformat(),
            "modified_time": datetime.now().isoformat(),
            "size": os.path.getsize(file_path)
        }
    
    def delete_library(self, name: str) -> bool:
        """åˆ é™¤æ•æ„Ÿè¯åº“"""
        filename = f"{name}.txt"
        file_path = os.path.join(self.base_path, filename)
        
        if not os.path.exists(file_path):
            raise HTTPException(status_code=404, detail=f"æ•æ„Ÿè¯åº“ '{name}' ä¸å­˜åœ¨")
        
        os.remove(file_path)
        return True
    
    def get_library_content(self, name: str) -> List[str]:
        """è·å–æ•æ„Ÿè¯åº“å†…å®¹"""
        filename = f"{name}.txt"
        file_path = os.path.join(self.base_path, filename)
        
        if not os.path.exists(file_path):
            raise HTTPException(status_code=404, detail=f"æ•æ„Ÿè¯åº“ '{name}' ä¸å­˜åœ¨")
        
        with open(file_path, "r", encoding="utf-8") as f:
            return [line.strip() for line in f if line.strip()]
    
    def update_library(self, name: str, words: List[str]) -> Dict[str, Any]:
        """æ›´æ–°æ•æ„Ÿè¯åº“"""
        filename = f"{name}.txt"
        file_path = os.path.join(self.base_path, filename)
        
        if not os.path.exists(file_path):
            raise HTTPException(status_code=404, detail=f"æ•æ„Ÿè¯åº“ '{name}' ä¸å­˜åœ¨")
        
        # å†™å…¥æ–‡ä»¶
        with open(file_path, "w", encoding="utf-8") as f:
            for word in words:
                f.write(word.strip() + "\n")
        
        return {
            "id": name,
            "name": name,
            "filename": filename,
            "path": file_path,
            "word_count": len(words),
            "modified_time": datetime.now().isoformat(),
            "size": os.path.getsize(file_path)
        }

# OCRé…ç½®å’Œé¢„å¤„ç†å‡½æ•°
def preprocess_image_for_ocr(image):
    """é¢„å¤„ç†å›¾ç‰‡ä»¥æé«˜OCRè¯†åˆ«ç‡"""
    # è½¬æ¢ä¸ºRGBæ¨¡å¼
    if image.mode != 'RGB':
        image = image.convert('RGB')
    
    # å¯ä»¥æ·»åŠ æ›´å¤šé¢„å¤„ç†æ­¥éª¤ï¼Œå¦‚ï¼š
    # - è°ƒæ•´å¯¹æ¯”åº¦
    # - å»å™ª
    # - äºŒå€¼åŒ–
    # - å€¾æ–œæ ¡æ­£ç­‰
    
    return image

def get_ocr_config():
    """è·å–OCRé…ç½®å‚æ•°"""
    return {
        'lang': 'chi_sim+eng',  # æ”¯æŒä¸­æ–‡ç®€ä½“å’Œè‹±æ–‡
        'config': '--psm 6 --oem 3',  # PSM 6: ç»Ÿä¸€æ–‡æœ¬å—, OEM 3: é»˜è®¤å¼•æ“
        'char_whitelist': None  # ç§»é™¤å­—ç¬¦ç™½åå•é™åˆ¶ï¼Œå…è®¸è¯†åˆ«æ‰€æœ‰å­—ç¬¦
    }

# åˆå§‹åŒ–æ•æ„Ÿè¯åº“ç®¡ç†å™¨
word_lib_manager = WordLibraryManager()

# æ£€æµ‹è¯åº“æŒä¹…åŒ–ç®¡ç†
class DetectionLibraryManager:
    """æ£€æµ‹è¯åº“æŒä¹…åŒ–ç®¡ç†å™¨"""
    
    def __init__(self, config_path="/app/detection_config.json"):
        self.config_path = config_path
        self.config = self.load_config()
    
    def load_config(self):
        """åŠ è½½æ£€æµ‹é…ç½®"""
        try:
            if os.path.exists(self.config_path):
                with open(self.config_path, "r", encoding="utf-8") as f:
                    return json.load(f)
        except Exception as e:
            print(f"åŠ è½½æ£€æµ‹é…ç½®å¤±è´¥: {e}")
        
        # é»˜è®¤é…ç½®
        return {
            "used_libraries": [],
            "last_updated": None,
            "word_count": 0
        }
    
    def save_config(self, used_libraries, word_count):
        """ä¿å­˜æ£€æµ‹é…ç½®"""
        try:
            config = {
                "used_libraries": used_libraries,
                "last_updated": datetime.now().isoformat(),
                "word_count": word_count
            }
            with open(self.config_path, "w", encoding="utf-8") as f:
                json.dump(config, f, ensure_ascii=False, indent=2)
            # æ›´æ–°å†…å­˜ä¸­çš„é…ç½®
            self.config = config
            print(f"æ£€æµ‹é…ç½®å·²ä¿å­˜: ä½¿ç”¨ {len(used_libraries)} ä¸ªè¯åº“ï¼Œå…± {word_count} ä¸ªæ•æ„Ÿè¯")
        except Exception as e:
            print(f"ä¿å­˜æ£€æµ‹é…ç½®å¤±è´¥: {e}")
    
    def reload_config(self):
        """é‡æ–°åŠ è½½é…ç½®æ–‡ä»¶"""
        print(f"å¼€å§‹é‡æ–°åŠ è½½é…ç½®æ–‡ä»¶: {self.config_path}")
        old_config = self.config.copy() if self.config else {}
        self.config = self.load_config()
        print(f"é‡æ–°åŠ è½½æ£€æµ‹é…ç½®å®Œæˆ:")
        print(f"  æ—§é…ç½®: {old_config}")
        print(f"  æ–°é…ç½®: {self.config}")
        print(f"  é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨: {os.path.exists(self.config_path)}")
    
    def get_used_libraries(self):
        """è·å–å½“å‰ä½¿ç”¨çš„è¯åº“åˆ—è¡¨"""
        return self.config.get("used_libraries", [])
    
    def get_word_count(self):
        """è·å–å½“å‰è¯åº“çš„æ•æ„Ÿè¯æ•°é‡"""
        return self.config.get("word_count", 0)

# åˆå§‹åŒ–æ£€æµ‹è¯åº“ç®¡ç†å™¨
detection_lib_manager = DetectionLibraryManager()

# æ¨¡å‹çŠ¶æ€è·Ÿè¸ª
model_warm_up_status = {
    "is_warmed_up": False,
    "warm_up_time": None,
    "last_call_time": None
}


# ---------------------- ä¸‰æ­¥åŒ¹é…è§„åˆ™å¼•æ“ ----------------------

# ç¬¬ä¸€æ­¥ï¼šACè‡ªåŠ¨æœºåˆç­› - å¿«é€Ÿè¿‡æ»¤æ— é£é™©æ–‡æœ¬ï¼Œæ ‡è®°å¯ç–‘æ–‡æœ¬
class ACNode:
    def __init__(self):
        self.children = {}
        self.fail = None
        self.is_end = False
        self.word = None
        self.output = []

class ACAutomaton:
    def __init__(self, words):
        self.root = ACNode()
        self.words = words
        # æ·»åŠ æ•æ„Ÿè¯åˆ°ACè‡ªåŠ¨æœº
        for word in words:
                        self.add_word(word)
        self.build_fail_links()

    def add_word(self, word):
        """æ·»åŠ æ•æ„Ÿè¯åˆ°ACè‡ªåŠ¨æœº"""
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = ACNode()
            node = node.children[char]
        node.is_end = True
        node.word = word
        node.output.append(word)

    def build_fail_links(self):
        """æ„å»ºå¤±è´¥é“¾æ¥"""
        from collections import deque
        queue = deque()
        
        # ç¬¬ä¸€å±‚èŠ‚ç‚¹çš„failæŒ‡é’ˆæŒ‡å‘root
        for child in self.root.children.values():
            child.fail = self.root
            queue.append(child)
        
        # æ„å»ºå…¶ä»–å±‚çš„failæŒ‡é’ˆ
        while queue:
            current = queue.popleft()
            for char, child in current.children.items():
                queue.append(child)
                fail_node = current.fail
                while fail_node and char not in fail_node.children:
                    fail_node = fail_node.fail
                if fail_node:
                    child.fail = fail_node.children.get(char, self.root)
                else:
                    child.fail = self.root
                # åˆå¹¶è¾“å‡º
                child.output.extend(child.fail.output)

    def search(self, text):
        """ACè‡ªåŠ¨æœºæœç´¢ï¼Œè¿”å›å¯ç–‘æ–‡æœ¬ç‰‡æ®µå’ŒåŒ¹é…çš„æ•æ„Ÿè¯"""
        results = []
        suspicious_segments = []
        current = self.root
        
        for i, char in enumerate(text):
            # æ²¿ç€failé“¾æ‰¾åˆ°åŒ¹é…çš„èŠ‚ç‚¹
            while current and char not in current.children:
                current = current.fail
            if current:
                current = current.children.get(char, self.root)
            else:
                current = self.root
            
            # æ£€æŸ¥å½“å‰èŠ‚ç‚¹åŠå…¶failé“¾ä¸Šçš„è¾“å‡º
            temp = current
            while temp:
                for word in temp.output:
                    results.append(word)
                    # æ ‡è®°å¯ç–‘æ–‡æœ¬ç‰‡æ®µï¼ˆå‘å‰æ‰©å±•ä¸€äº›å­—ç¬¦ä»¥æ•è·ä¸Šä¸‹æ–‡ï¼‰
                    start = max(0, i - len(word) - 5)
                    end = min(len(text), i + 5)
                    suspicious_segments.append(text[start:end])
                temp = temp.fail
        
        return list(set(results)), list(set(suspicious_segments))

# ç¬¬äºŒæ­¥ï¼šDFAæ£€æµ‹ - å¯¹å¯ç–‘æ–‡æœ¬è¿›è¡Œç²¾å‡†éªŒè¯
class DFAFilter:
    def __init__(self, words):
        self.words = words
        self.dfa = self.build_dfa()

    def build_dfa(self):
        """æ„å»ºDFAçŠ¶æ€æœº"""
        dfa = {}
        state = 0
        
        for word in self.words:
            current_state = 0
            for char in word:
                if (current_state, char) not in dfa:
                    state += 1
                    dfa[(current_state, char)] = state
                current_state = dfa[(current_state, char)]
            # æ ‡è®°ç»ˆæ€
            dfa[(current_state, '')] = -1  # -1è¡¨ç¤ºç»ˆæ€
        
        return dfa

    def precise_match(self, text, suspicious_segments):
        """å¯¹å¯ç–‘æ–‡æœ¬ç‰‡æ®µè¿›è¡ŒDFAç²¾å‡†åŒ¹é…"""
        precise_results = []
        
        for segment in suspicious_segments:
            for i in range(len(segment)):
                current_state = 0
                for j in range(i, len(segment)):
                    char = segment[j]
                    if (current_state, char) in self.dfa:
                        current_state = self.dfa[(current_state, char)]
                        if (current_state, '') in self.dfa:  # åˆ°è¾¾ç»ˆæ€
                            precise_results.append(segment[i:j+1])
                    else:
                        break
        
        return list(set(precise_results))

# æ–‡æœ¬é¢„å¤„ç† - ç»Ÿä¸€å­—ç¬¦æ ¼å¼ï¼Œæ¶ˆé™¤"æ— æ„ä¹‰å˜ä½“"
class TextPreprocessor:
    def __init__(self):
        """æ–‡æœ¬é¢„å¤„ç†å™¨ï¼Œç”¨äºç»Ÿä¸€å­—ç¬¦æ ¼å¼ï¼Œæ¶ˆé™¤æ— æ„ä¹‰å˜ä½“"""
        self.setup_normalization_rules()
    
    def setup_normalization_rules(self):
        """è®¾ç½®å­—ç¬¦å½’ä¸€åŒ–è§„åˆ™"""
        # å…¨è§’è½¬åŠè§’æ˜ å°„
        self.full_to_half = {
            'ï¼¡': 'A', 'ï¼¢': 'B', 'ï¼£': 'C', 'ï¼¤': 'D', 'ï¼¥': 'E', 'ï¼¦': 'F', 'ï¼§': 'G', 'ï¼¨': 'H',
            'ï¼©': 'I', 'ï¼ª': 'J', 'ï¼«': 'K', 'ï¼¬': 'L', 'ï¼­': 'M', 'ï¼®': 'N', 'ï¼¯': 'O', 'ï¼°': 'P',
            'ï¼±': 'Q', 'ï¼²': 'R', 'ï¼³': 'S', 'ï¼´': 'T', 'ï¼µ': 'U', 'ï¼¶': 'V', 'ï¼·': 'W', 'ï¼¸': 'X',
            'ï¼¹': 'Y', 'ï¼º': 'Z',
            'ï½': 'a', 'ï½‚': 'b', 'ï½ƒ': 'c', 'ï½„': 'd', 'ï½…': 'e', 'ï½†': 'f', 'ï½‡': 'g', 'ï½ˆ': 'h',
            'ï½‰': 'i', 'ï½Š': 'j', 'ï½‹': 'k', 'ï½Œ': 'l', 'ï½': 'm', 'ï½': 'n', 'ï½': 'o', 'ï½': 'p',
            'ï½‘': 'q', 'ï½’': 'r', 'ï½“': 's', 'ï½”': 't', 'ï½•': 'u', 'ï½–': 'v', 'ï½—': 'w', 'ï½˜': 'x',
            'ï½™': 'y', 'ï½š': 'z',
            'ï¼': '0', 'ï¼‘': '1', 'ï¼’': '2', 'ï¼“': '3', 'ï¼”': '4', 'ï¼•': '5', 'ï¼–': '6', 'ï¼—': '7',
            'ï¼˜': '8', 'ï¼™': '9',
            'ï¼ˆ': '(', 'ï¼‰': ')', 'ï¼»': '[', 'ï¼½': ']', 'ï½›': '{', 'ï½': '}', 'ï¼œ': '<', 'ï¼': '>',
            'ï¼Œ': ',', 'ã€‚': '.', 'ï¼›': ';', 'ï¼š': ':', 'ï¼Ÿ': '?', 'ï¼': '!', 'ï½': '~', 'ï¼ ': '@',
            'ï¼ƒ': '#', 'ï¼„': '$', 'ï¼…': '%', 'ï¼¾': '^', 'ï¼†': '&', 'ï¼Š': '*', 'ï¼': '-', 'ï¼¿': '_',
            'ï¼‹': '+', 'ï¼': '=', 'ï½œ': '|', 'ï¼¼': '\\', 'ï¼': '/', 'ã€€': ' '
        }
        
        # ç¹ä½“è½¬ç®€ä½“æ˜ å°„ï¼ˆå¸¸ç”¨å­—ï¼‰
        self.traditional_to_simplified = {
            'å­¸': 'å­¦', 'ç¿’': 'ä¹ ', 'ç¶“': 'ç»', 'æ¿Ÿ': 'æµ', 'ç™¼': 'å‘', 'ç¾': 'ç°', 'å¯¦': 'å®',
            'éš›': 'é™…', 'é›»': 'ç”µ', 'è…¦': 'è„‘', 'ç¶²': 'ç½‘', 'çµ¡': 'ç»œ', 'è³‡': 'èµ„', 'è¨Š': 'è®¯',
            'è©±': 'è¯', 'è¦–': 'è§†', 'å½±': 'å½±', 'è»Ÿ': 'è½¯', 'é«”': 'ä½“', 'ç¡¬': 'ç¡¬', 'ç³»': 'ç³»',
            'çµ±': 'ç»Ÿ', 'ä»¶': 'ä»¶', 'ç¨‹': 'ç¨‹', 'å¼': 'å¼', 'è¨­': 'è®¾', 'è¨ˆ': 'è®¡', 'é–‹': 'å¼€',
            'æ¸¬': 'æµ‹', 'è©¦': 'è¯•', 'ç¶­': 'ç»´', 'è­·': 'æŠ¤', 'ç®¡': 'ç®¡', 'ç†': 'ç†', 'æœ': 'æœ',
            'å‹™': 'åŠ¡', 'ç”¢': 'äº§', 'å“': 'å“', 'è³ª': 'è´¨', 'é‡': 'é‡', 'æ¨™': 'æ ‡', 'æº–': 'å‡†',
            'è¦': 'è§„', 'ç¯„': 'èŒƒ', 'èªŒ': 'å¿—', 'è­˜': 'è¯†', 'åˆ¥': 'åˆ«', 'å€': 'åŒº', 'åˆ†': 'åˆ†',
            'é¡': 'ç±»', 'å‹': 'å‹', 'ç¨®': 'ç§', 'ç´š': 'çº§', 'å±¤': 'å±‚', 'æ¬¡': 'æ¬¡', 'é †': 'é¡º',
            'åº': 'åº', 'æ’': 'æ’', 'åˆ—': 'åˆ—', 'çµ„': 'ç»„', 'åˆ': 'åˆ', 'é…': 'é…', 'ç½®': 'ç½®',
            'å®š': 'å®š', 'é¸': 'é€‰', 'æ“‡': 'æ‹©', 'æ±º': 'å†³', 'ç¢º': 'ç¡®', 'èª': 'è®¤', 'è­‰': 'è¯',
            'é©—': 'éªŒ', 'æ˜': 'æ˜', 'æ“š': 'æ®', 'æ›¸': 'ä¹¦', 'ç…§': 'ç…§', 'åˆ¸': 'åˆ¸', 'ç¥¨': 'ç¥¨'
        }
    
    def normalize_text(self, text):
        """æ–‡æœ¬å½’ä¸€åŒ–å¤„ç†"""
        if not text:
            return text
        
        # 1. å…¨è§’è½¬åŠè§’
        normalized = text
        for full_char, half_char in self.full_to_half.items():
            normalized = normalized.replace(full_char, half_char)
        
        # 2. ç¹ä½“è½¬ç®€ä½“
        for trad_char, simp_char in self.traditional_to_simplified.items():
            normalized = normalized.replace(trad_char, simp_char)
        
        # 3. ç§»é™¤ç‰¹æ®Šç¬¦å·ï¼ˆä¿ç•™ä¸­æ–‡å­—ç¬¦ã€è‹±æ–‡å­—æ¯ã€æ•°å­—ï¼‰
        cleaned = ""
        for char in normalized:
            if (char.isalnum() or  # å­—æ¯æˆ–æ•°å­—
                '\u4e00' <= char <= '\u9fff'):  # ä¸­æ–‡å­—ç¬¦
                cleaned += char
        
        return cleaned
    
    def preprocess_text(self, text):
        """é¢„å¤„ç†æ–‡æœ¬ï¼Œè¿”å›å½’ä¸€åŒ–æ–‡æœ¬"""
        return self.normalize_text(text)

# è§„åˆ™åŒ¹é…å¼•æ“æ•´åˆï¼ˆé¢„å¤„ç†+AC+DFAï¼‰
class ThreeStepFilter:
    def __init__(self, word_paths=None):
        if word_paths is None:
            # é»˜è®¤ä½¿ç”¨word_librariesç›®å½•ä¸­çš„æ‰€æœ‰è¯åº“
            word_paths = []
            base_path = "/app/word_libraries"
            if os.path.exists(base_path):
                for filename in os.listdir(base_path):
                    if filename.endswith('.txt'):
                        word_paths.append(os.path.join(base_path, filename))
        self.word_paths = word_paths
        self.words = []
        self._load_words()
        # æ–‡æœ¬é¢„å¤„ç†å™¨
        self.text_preprocessor = TextPreprocessor()
        # ç¬¬ä¸€æ­¥ï¼šACè‡ªåŠ¨æœº
        self.ac_automaton = ACAutomaton(self.words)
        # ç¬¬äºŒæ­¥ï¼šDFAæ£€æµ‹
        self.dfa_filter = DFAFilter(self.words)

    def _load_words(self):
        """åŠ è½½æ•æ„Ÿè¯å¹¶è‡ªåŠ¨å»é‡"""
        all_words = []
        word_sources = {}  # è®°å½•æ¯ä¸ªè¯æ¥è‡ªå“ªäº›è¯åº“
        
        for word_path in self.word_paths:
            if os.path.exists(word_path):
                library_name = os.path.splitext(os.path.basename(word_path))[0]
                with open(word_path, "r", encoding="utf-8") as f:
                    for word in f:
                        word = word.strip()
                        if word:
                            all_words.append(word)
                            # è®°å½•è¯åº“æ¥æº
                            if word not in word_sources:
                                word_sources[word] = []
                            word_sources[word].append(library_name)
            else:
                print(f"è­¦å‘Šï¼šæ•æ„Ÿè¯åº“æ–‡ä»¶ {word_path} ä¸å­˜åœ¨")
        
        # å»é‡å¹¶ç»Ÿè®¡
        original_count = len(all_words)
        self.words = list(set(all_words))  # è‡ªåŠ¨å»é‡
        deduplicated_count = len(self.words)
        removed_count = original_count - deduplicated_count
        
        # æ‰“å°å»é‡ç»Ÿè®¡ä¿¡æ¯
        if removed_count > 0:
            print(f"è¯åº“å»é‡å®Œæˆï¼šåŸå§‹ {original_count} ä¸ªè¯ï¼Œå»é‡å {deduplicated_count} ä¸ªè¯ï¼Œåˆ é™¤é‡å¤è¯ {removed_count} ä¸ª")
            
            # æ˜¾ç¤ºé‡å¤è¯åŠå…¶æ¥æºï¼ˆä»…æ˜¾ç¤ºå‰10ä¸ªï¼‰
            duplicates = [word for word, sources in word_sources.items() if len(sources) > 1]
            if duplicates:
                print("é‡å¤è¯ç¤ºä¾‹ï¼ˆå‰10ä¸ªï¼‰:")
                for i, word in enumerate(duplicates[:10]):
                    sources = word_sources[word]
                    print(f"  '{word}' å‡ºç°åœ¨: {', '.join(sources)}")
                if len(duplicates) > 10:
                    print(f"  ... è¿˜æœ‰ {len(duplicates) - 10} ä¸ªé‡å¤è¯")
        else:
            print(f"è¯åº“åŠ è½½å®Œæˆï¼šå…± {deduplicated_count} ä¸ªè¯ï¼Œæ— é‡å¤è¯")

    def reload_with_libraries(self, library_names: List[str]):
        """é‡æ–°åŠ è½½æŒ‡å®šçš„æ•æ„Ÿè¯åº“"""
        word_paths = []
        for name in library_names:
            file_path = os.path.join(word_lib_manager.base_path, f"{name}.txt")
            if os.path.exists(file_path):
                word_paths.append(file_path)
        
        if word_paths:
            self.word_paths = word_paths
            self._load_words()
            # é‡æ–°åˆå§‹åŒ–å„ä¸ªç»„ä»¶
            self.text_preprocessor = TextPreprocessor()
            self.ac_automaton = ACAutomaton(self.words)
            self.dfa_filter = DFAFilter(self.words)

    def detect(self, text, fast_mode=False):
        """è§„åˆ™åŒ¹é…æ£€æµ‹ï¼ˆé¢„å¤„ç†+AC+DFAï¼‰"""
        start_time = time.time()
        
        # æ–‡æœ¬é¢„å¤„ç†ï¼šå½’ä¸€åŒ–å­—ç¬¦æ ¼å¼
        preprocess_start = time.time()
        normalized_text = self.text_preprocessor.preprocess_text(text)
        preprocess_time = time.time() - preprocess_start
        
        # ç¬¬ä¸€æ­¥ï¼šACè‡ªåŠ¨æœºåˆç­›ï¼ˆå¯¹å½’ä¸€åŒ–æ–‡æœ¬ï¼‰
        ac_start = time.time()
        ac_results, suspicious_segments = self.ac_automaton.search(normalized_text)
        ac_time = time.time() - ac_start
        
        # ç¬¬äºŒæ­¥ï¼šDFAæ£€æµ‹ï¼ˆå¯¹åŸå§‹æ–‡æœ¬çš„å¯ç–‘ç‰‡æ®µï¼‰
        dfa_start = time.time()
        dfa_results = self.dfa_filter.precise_match(text, suspicious_segments)
        dfa_time = time.time() - dfa_start
        
        # åˆå¹¶æ‰€æœ‰ç»“æœ
        all_results = list(set(ac_results + dfa_results))
        
        total_time = time.time() - start_time
        
        return {
            'ac_results': ac_results,
            'dfa_results': dfa_results,
            'preprocess_results': [],  # é¢„å¤„ç†ç»“æœï¼ˆç”¨äºå…¼å®¹æ€§ï¼‰
            'all_results': all_results,
            'suspicious_segments': suspicious_segments,
            'word_count': len(self.words),  # æ·»åŠ è¯åº“ç»Ÿè®¡ä¿¡æ¯
            'fast_mode': fast_mode,  # æ˜¯å¦ä½¿ç”¨å¿«é€Ÿæ¨¡å¼
            'normalized_text': normalized_text,  # å½’ä¸€åŒ–åçš„æ–‡æœ¬
            'timing': {
                'preprocess_time': round(preprocess_time * 1000, 2),  # é¢„å¤„ç†ç”¨æ—¶
                'ac_time': round(ac_time * 1000, 2),      # æ¯«ç§’
                'dfa_time': round(dfa_time * 1000, 2),    # æ¯«ç§’
                'total_time': round(total_time * 1000, 2)  # æ¯«ç§’
            }
        }

# åˆå§‹åŒ–ä¸‰æ­¥åŒ¹é…è§„åˆ™å¼•æ“ï¼ˆåŠ è½½æ•æ„Ÿè¯åº“ï¼‰
# é»˜è®¤ä½¿ç”¨word_librariesä¸­çš„è¯åº“

# æ¨¡å‹é¢„çƒ­å‡½æ•°

def call_ollama_api(text: str) -> str:
    """
    è°ƒç”¨ Ollama æœ¬åœ° APIï¼Œæ£€æµ‹æ–‡æœ¬æ˜¯å¦å«æ•æ„Ÿå†…å®¹
    è¿”å›ï¼š"æ•æ„Ÿ" æˆ– "æ­£å¸¸"ï¼ˆå®¹é”™å¤„ç†åï¼‰
    """
    
    # è®°å½•è°ƒç”¨æ—¶é—´
    call_start_time = time.time()
    model_warm_up_status["last_call_time"] = call_start_time
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯å†·å¯åŠ¨
    if not model_warm_up_status["is_warmed_up"]:
        print("âš ï¸  æ£€æµ‹åˆ°æ¨¡å‹å†·å¯åŠ¨ï¼Œå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´...")
    elif model_warm_up_status["warm_up_time"]:
        time_since_warmup = call_start_time - model_warm_up_status["warm_up_time"]
        if time_since_warmup > 300:  # 5åˆ†é’Ÿåè®¤ä¸ºå¯èƒ½å†·å¯åŠ¨
            print(f"âš ï¸  è·ç¦»é¢„çƒ­å·²è¿‡{time_since_warmup:.0f}ç§’ï¼Œå¯èƒ½è§¦å‘å†·å¯åŠ¨...")
    
    # Ollama API åœ°å€ä¸æ¨¡å‹åé€šè¿‡ç¯å¢ƒå˜é‡é…ç½®
    # åœ¨WSLç¯å¢ƒä¸­ï¼Œä½¿ç”¨host.docker.internalå¯èƒ½æ— æ³•è§£æï¼Œå°è¯•å¤šç§æ–¹å¼
    base_url = os.getenv("OLLAMA_BASE_URL", "http://172.17.0.1:11434").rstrip("/")
    model_name = os.getenv("OLLAMA_MODEL", "qwen2.5:7b-instruct-q4_K_M")
    ollama_url = f"{base_url}/api/generate"
    
    print(f"å°è¯•è°ƒç”¨Ollama API: {ollama_url}")
    print(f"ä½¿ç”¨æ¨¡å‹: {model_name}")
    
    # æç¤ºè¯å·¥ç¨‹ï¼šä¸¥æ ¼çº¦æŸè¾“å‡ºï¼Œç¡®ä¿ä¸€è‡´æ€§
    prompt = f"""
ä½ æ˜¯ä¸“ä¸šçš„å†…å®¹å®‰å…¨æ£€æµ‹ç³»ç»Ÿã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è§„åˆ™è¿›è¡Œåˆ¤æ–­ï¼š

ã€æ•æ„Ÿå†…å®¹æ ‡å‡†ã€‘
åŒ…å«ä»¥ä¸‹ä»»ä½•ä¸€ç±»å†…å®¹å³åˆ¤å®šä¸ºæ•æ„Ÿï¼š
1. è¾±éª‚è¯æ±‡ï¼ˆå¦‚ï¼šç™½ç—´ã€å‚»é€¼ã€å»æ­»ç­‰ï¼‰
2. æš´åŠ›å¨èƒï¼ˆå¦‚ï¼šæ€äº†ä½ ã€æ‰“ä½ ã€ä¼¤å®³ç­‰ï¼‰
3. è¿æ³•å†…å®¹ï¼ˆå¦‚ï¼šæ¯’å“ã€çŠ¯ç½ªã€è¯ˆéª—ç­‰ï¼‰
4. è‰²æƒ…å†…å®¹ï¼ˆå¦‚ï¼šæ€§æš—ç¤ºã€è‰²æƒ…æè¿°ç­‰ï¼‰
5. æ­§è§†å†…å®¹ï¼ˆå¦‚ï¼šç§æ—æ­§è§†ã€æ€§åˆ«æ­§è§†ç­‰ï¼‰
6. æ”¿æ²»æ•æ„Ÿå†…å®¹ï¼ˆå¦‚ï¼šæ³•è¥¿æ–¯ã€çº³ç²¹ã€æç«¯ä¸»ä¹‰ã€ææ€–ä¸»ä¹‰ã€åˆ†è£‚ä¸»ä¹‰ç­‰ï¼‰
7. ä»‡æ¨è¨€è®ºï¼ˆå¦‚ï¼šç…½åŠ¨ä»‡æ¨ã€æš´åŠ›ã€æç«¯çš„è¨€è®ºï¼‰
8. è™šå‡ä¿¡æ¯ï¼ˆå¦‚ï¼šè™šå‡æ–°é—»ã€è™šå‡å®£ä¼ ã€è™šå‡å¹¿å‘Šã€è°£è¨€ç­‰ï¼‰
9. æ¬ºå‡Œè¨€è®ºï¼ˆå¦‚ï¼šéœ¸å‡Œã€å†·æš´åŠ›ã€å­¤ç«‹çš„è¨€è®ºï¼‰
10. éšè”½çš„æ•æ„Ÿå†…å®¹ï¼ˆå¦‚ï¼šååŠ¨å†…å®¹ã€è‰²æƒ…å†…å®¹ã€æš´åŠ›å†…å®¹ã€è¿æ³•å†…å®¹ç­‰ï¼Œä½†éœ€è¦é€šè¿‡ä¸Šä¸‹æ–‡æ‰èƒ½åˆ¤æ–­ï¼‰
ã€æ£€æµ‹æ–‡æœ¬ã€‘
{text[:10000]}

ã€åˆ¤æ–­è§„åˆ™ã€‘
- å¦‚æœæ–‡æœ¬åŒ…å«ä¸Šè¿°ä»»ä½•ä¸€ç±»æ•æ„Ÿå†…å®¹ï¼Œå›ç­”ï¼šæ•æ„Ÿ
- å¦‚æœæ–‡æœ¬æ˜¯æ­£å¸¸çš„æ—¥å¸¸å¯¹è¯ã€å­¦æœ¯è®¨è®ºã€æ–°é—»æè¿°ç­‰ï¼Œå›ç­”ï¼šæ­£å¸¸
- å¯¹äºæ¶‰åŠæ•æ„Ÿè¯æ±‡ä½†å±äºå­¦æœ¯ç ”ç©¶ã€å†å²è®¨è®ºã€æ–°é—»æŠ¥é“ç­‰æ­£å½“ç”¨é€”ï¼Œå›ç­”ï¼šæ­£å¸¸
- å¯¹äºæ˜ç¡®è¡¨è¾¾æ”¯æŒã€å®£æ‰¬ã€ç¾åŒ–æ•æ„Ÿå†…å®¹çš„ï¼Œå›ç­”ï¼šæ•æ„Ÿ

ã€è¾“å‡ºè¦æ±‚ã€‘
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ï¼š
æ•æ„Ÿ
æˆ–
æ­£å¸¸
    """
    
    try:
        # å‘é€ POST è¯·æ±‚åˆ° Ollama API
        print(f"å‘é€è¯·æ±‚åˆ°: {ollama_url}")
        response = requests.post(
            ollama_url,
            json={
                "model": model_name,
                "prompt": prompt,
                "stream": False,
                "temperature": 0
            },
            timeout=30
        )
        print(f"APIå“åº”çŠ¶æ€ç : {response.status_code}")
        response.raise_for_status()  # è‹¥ HTTP çŠ¶æ€ç å¼‚å¸¸ï¼ŒæŠ›å‡ºé”™è¯¯
        result = response.json()
        print(f"APIå“åº”å†…å®¹: {result}")
        
        # æå–æ¨¡å‹å“åº”ï¼Œæ¸…ç†ç©ºæ ¼å’Œæ¢è¡Œ
        llm_output = result.get("response", "").strip()
        print(f"æ¨¡å‹è¾“å‡º: '{llm_output}'")
        # å®¹é”™å¤„ç†ï¼šè‹¥æ¨¡å‹è¾“å‡ºå¼‚å¸¸ï¼Œé»˜è®¤è¿”å›"æ­£å¸¸"
        final_result = llm_output if llm_output in ["æ•æ„Ÿ", "æ­£å¸¸"] else "æ­£å¸¸"
        print(f"æœ€ç»ˆç»“æœ: {final_result}")
        return final_result
    
    except Exception as e:
        # æ•è·ç½‘ç»œé”™è¯¯ã€API é”™è¯¯ç­‰ï¼Œæ‰“å°æ—¥å¿—å¹¶è¿”å›"æ­£å¸¸"ï¼ˆé¿å…æœåŠ¡å´©æºƒï¼‰
        print(f"Ollama API è°ƒç”¨å¤±è´¥ï¼š{str(e)}")
        print(f"å¼‚å¸¸ç±»å‹: {type(e).__name__}")
        return "æ­£å¸¸"

def warm_up_model():
    """é¢„çƒ­Ollamaæ¨¡å‹ï¼Œé¿å…ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶çš„å†·å¯åŠ¨å»¶è¿Ÿ"""
    try:
        print("æ­£åœ¨é¢„çƒ­Ollamaæ¨¡å‹...")
        
        # å¤šæ¬¡é¢„çƒ­ï¼Œç¡®ä¿æ¨¡å‹å®Œå…¨åŠ è½½
        warm_up_texts = [
            "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ï¼Œç”¨äºé¢„çƒ­æ¨¡å‹ã€‚",
            "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé€‚åˆå‡ºé—¨æ•£æ­¥ã€‚",
            "è¯·å¸®æˆ‘åˆ†æä¸€ä¸‹è¿™ä¸ªæ–‡æœ¬çš„å†…å®¹ã€‚"
        ]
        
        for i, text in enumerate(warm_up_texts, 1):
            print(f"é¢„çƒ­ç¬¬{i}æ¬¡...")
            result = call_ollama_api(text)
            print(f"ç¬¬{i}æ¬¡é¢„çƒ­å®Œæˆï¼Œç»“æœ: {result}")
            
            # çŸ­æš‚å»¶è¿Ÿï¼Œè®©æ¨¡å‹ç¨³å®š
            time.sleep(0.5)
        
        print("æ¨¡å‹é¢„çƒ­å…¨éƒ¨å®Œæˆï¼")
        
        # æ›´æ–°é¢„çƒ­çŠ¶æ€
        model_warm_up_status["is_warmed_up"] = True
        model_warm_up_status["warm_up_time"] = time.time()
        
        return True
    except Exception as e:
        print(f"æ¨¡å‹é¢„çƒ­å¤±è´¥: {e}")
        return False


# å¯åŠ¨æ—¶åŠ è½½ä¿å­˜çš„æ£€æµ‹è¯åº“é…ç½®
def initialize_detection_filter():
    """åˆå§‹åŒ–æ£€æµ‹è¿‡æ»¤å™¨"""
    used_libraries = detection_lib_manager.get_used_libraries()
    
    # é¢„çƒ­Ollamaæ¨¡å‹
    warm_up_model()
    
    if used_libraries:
        print(f"åŠ è½½ä¿å­˜çš„æ£€æµ‹è¯åº“é…ç½®: {', '.join(used_libraries)}")
        # ä½¿ç”¨ä¿å­˜çš„è¯åº“é…ç½®
        word_paths = []
        for name in used_libraries:
            file_path = os.path.join(word_lib_manager.base_path, f"{name}.txt")
            if os.path.exists(file_path):
                word_paths.append(file_path)
            else:
                print(f"è­¦å‘Šï¼šä¿å­˜çš„è¯åº“ '{name}' æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡")
        
        if word_paths:
            return ThreeStepFilter(word_paths)
        else:
            print("æ‰€æœ‰ä¿å­˜çš„è¯åº“æ–‡ä»¶éƒ½ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤è¯åº“")
    
    # ä½¿ç”¨word_librariesä¸­çš„è¯åº“ä½œä¸ºé»˜è®¤è¯åº“
    print("ä½¿ç”¨word_librariesä¸­çš„è¯åº“ä½œä¸ºé»˜è®¤è¯åº“")
    word_lib_paths = []
    
    # æ‰«æword_librariesç›®å½•ä¸­çš„æ‰€æœ‰è¯åº“
    if os.path.exists(word_lib_manager.base_path):
        for filename in os.listdir(word_lib_manager.base_path):
            if filename.endswith('.txt'):
                file_path = os.path.join(word_lib_manager.base_path, filename)
                word_lib_paths.append(file_path)
                print(f"æ‰¾åˆ°è¯åº“: {filename}")
    
    if word_lib_paths:
        print(f"ä½¿ç”¨ {len(word_lib_paths)} ä¸ªè¯åº“ä½œä¸ºé»˜è®¤è¯åº“")
        return ThreeStepFilter(word_lib_paths)
    else:
        print("word_librariesç›®å½•ä¸ºç©ºï¼Œåˆ›å»ºé»˜è®¤è¯åº“")
        # åˆ›å»ºé»˜è®¤è¯åº“
        default_library_path = os.path.join(word_lib_manager.base_path, "é»˜è®¤è¯åº“.txt")
        with open(default_library_path, "w", encoding="utf-8") as f:
            f.write("æš´åŠ›\nè¾±éª‚\nè¿æ³•\nè‰²æƒ…\nèµŒåš\næ¯’å“\næ³•è¥¿æ–¯\nçº³ç²¹\næç«¯ä¸»ä¹‰\nææ€–ä¸»ä¹‰\n")
        print(f"å·²åˆ›å»ºé»˜è®¤è¯åº“: {default_library_path}")
        return ThreeStepFilter([default_library_path])

three_step_filter = initialize_detection_filter()

# ---------------------- æ–°å¢ï¼šOllama API è°ƒç”¨é€»è¾‘ ----------------------


# ---------------------- å®šä¹‰è¯·æ±‚å‚æ•°æ ¼å¼ ----------------------
class TextRequest(BaseModel):
    """æ–‡æœ¬æ£€æµ‹çš„è¯·æ±‚ä½“æ ¼å¼ï¼šå¿…é¡»åŒ…å«textå­—æ®µ"""
    text: str
    fast_mode: Optional[bool] = False  # å¿«é€Ÿæ¨¡å¼ï¼šå·²åºŸå¼ƒï¼Œå»ºè®®ä½¿ç”¨é»˜è®¤æ¨¡å¼
    strict_mode: Optional[bool] = False  # ä¸¥æ ¼æ¨¡å¼ï¼šè·³è¿‡è§„åˆ™åŒ¹é…ï¼Œç›´æ¥ä½¿ç”¨å¤§æ¨¡å‹

class LibraryCreateRequest(BaseModel):
    """åˆ›å»ºæ•æ„Ÿè¯åº“çš„è¯·æ±‚ä½“æ ¼å¼"""
    name: str
    words: List[str]

class LibraryUpdateRequest(BaseModel):
    """æ›´æ–°æ•æ„Ÿè¯åº“çš„è¯·æ±‚ä½“æ ¼å¼"""
    words: List[str]

# ---------------------- æ•æ„Ÿè¯åº“ç®¡ç†API ----------------------
@app.get("/word-libraries", summary="è·å–æ•æ„Ÿè¯åº“åˆ—è¡¨")
async def get_word_libraries():
    """è·å–æ‰€æœ‰æ•æ„Ÿè¯åº“åˆ—è¡¨"""
    libraries = word_lib_manager.get_library_list()
    return {
        "status": "success",
        "data": libraries
    }

@app.post("/word-libraries", summary="åˆ›å»ºæ•æ„Ÿè¯åº“")
async def create_word_library(req: LibraryCreateRequest):
    """åˆ›å»ºæ–°çš„æ•æ„Ÿè¯åº“"""
    if not req.name.strip():
        raise HTTPException(status_code=400, detail="è¯åº“åç§°ä¸èƒ½ä¸ºç©º")
    
    if not req.words:
        raise HTTPException(status_code=400, detail="æ•æ„Ÿè¯åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
    
    library = word_lib_manager.create_library(req.name.strip(), req.words)
    return {
        "status": "success",
        "data": library
    }

@app.get("/word-libraries/{name}", summary="è·å–æ•æ„Ÿè¯åº“å†…å®¹")
async def get_word_library_content(name: str):
    """è·å–æŒ‡å®šæ•æ„Ÿè¯åº“çš„å†…å®¹"""
    words = word_lib_manager.get_library_content(name)
    return {
        "status": "success",
        "data": {
            "name": name,
            "words": words,
            "word_count": len(words)
        }
    }

@app.put("/word-libraries/{name}", summary="æ›´æ–°æ•æ„Ÿè¯åº“")
async def update_word_library(name: str, req: LibraryUpdateRequest):
    """æ›´æ–°æŒ‡å®šæ•æ„Ÿè¯åº“çš„å†…å®¹"""
    if not req.words:
        raise HTTPException(status_code=400, detail="æ•æ„Ÿè¯åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
    
    library = word_lib_manager.update_library(name, req.words)
    return {
        "status": "success",
        "data": library
    }

@app.delete("/word-libraries/{name}", summary="åˆ é™¤æ•æ„Ÿè¯åº“")
async def delete_word_library(name: str):
    """åˆ é™¤æŒ‡å®šçš„æ•æ„Ÿè¯åº“"""
    word_lib_manager.delete_library(name)
    return {
        "status": "success",
        "message": f"æ•æ„Ÿè¯åº“ '{name}' å·²åˆ é™¤"
    }

@app.post("/detection-libraries/update", summary="æ›´æ–°æ£€æµ‹è¯åº“é…ç½®")
async def update_detection_libraries(req: dict):
    """æ›´æ–°æ£€æµ‹è¯åº“é…ç½®"""
    library_names = req.get("library_names", [])
    
    if not library_names:
        # æ¸…ç©ºæ£€æµ‹è¯åº“é…ç½®ï¼Œä½¿ç”¨é»˜è®¤è¯åº“
        detection_lib_manager.save_config([], 0)
        three_step_filter.reload_with_libraries([])
        return {
            "status": "success",
            "message": "å·²æ¸…ç©ºæ£€æµ‹è¯åº“é…ç½®ï¼Œä½¿ç”¨é»˜è®¤è¯åº“",
            "data": {
                "used_libraries": [],
                "word_count": len(three_step_filter.words)
            }
        }
    
    # éªŒè¯è¯åº“æ˜¯å¦å­˜åœ¨
    valid_libraries = []
    for name in library_names:
        file_path = os.path.join(word_lib_manager.base_path, f"{name}.txt")
        if os.path.exists(file_path):
            valid_libraries.append(name)
        else:
            print(f"è­¦å‘Šï¼šè¯åº“ '{name}' ä¸å­˜åœ¨ï¼Œè·³è¿‡")
    
    if not valid_libraries:
        return {
            "status": "error",
            "message": "æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è¯åº“"
        }
    
    # æ›´æ–°æ£€æµ‹è¯åº“
    three_step_filter.reload_with_libraries(valid_libraries)
    
    # ä¿å­˜é…ç½®
    detection_lib_manager.save_config(valid_libraries, len(three_step_filter.words))
    
    return {
        "status": "success",
        "message": f"æ£€æµ‹è¯åº“å·²æ›´æ–°ï¼Œä½¿ç”¨ {len(valid_libraries)} ä¸ªè¯åº“",
        "data": {
            "used_libraries": valid_libraries,
            "word_count": len(three_step_filter.words)
        }
    }

@app.get("/detection-libraries/status", summary="è·å–æ£€æµ‹è¯åº“çŠ¶æ€")
async def get_detection_libraries_status():
    """è·å–å½“å‰æ£€æµ‹è¯åº“çŠ¶æ€"""
    print("=== APIè°ƒç”¨: è·å–æ£€æµ‹è¯åº“çŠ¶æ€ ===")
    
    # å¼ºåˆ¶é‡æ–°åŠ è½½é…ç½®æ–‡ä»¶
    try:
        with open("/app/detection_config.json", "r", encoding="utf-8") as f:
            config = json.load(f)
        print(f"ç›´æ¥ä»æ–‡ä»¶è¯»å–é…ç½®: {config}")
        
        used_libraries = config.get("used_libraries", [])
        word_count = config.get("word_count", 0)
        last_updated = config.get("last_updated")
        
        print(f"è¿”å›çš„è¯åº“: {used_libraries}")
        print(f"è¿”å›çš„è¯åº“æ•°é‡: {len(used_libraries)}")
        print("=== APIè°ƒç”¨ç»“æŸ ===")
        
        return {
            "status": "success",
            "data": {
                "used_libraries": used_libraries,
                "word_count": word_count,
                "last_updated": last_updated
            }
        }
    except Exception as e:
        print(f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        return {
            "status": "error",
            "message": f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}"
        }

# ---------------------- æ ¸å¿ƒAPIï¼šæ–‡æœ¬æ£€æµ‹ ----------------------
@app.post("/detect/text", summary="æ–‡æœ¬æ•æ„Ÿè¯æ£€æµ‹")
async def detect_text(req: TextRequest):
    # 1. æ ¡éªŒè¯·æ±‚å‚æ•°ï¼ˆæ–‡æœ¬ä¸èƒ½ä¸ºç©ºï¼‰
    if not req.text.strip():
        raise HTTPException(status_code=400, detail="æ£€æµ‹æ–‡æœ¬ä¸èƒ½ä¸ºç©º")
    
    # è°ƒè¯•æ—¥å¿—
    print(f"ğŸ” è°ƒè¯•ä¿¡æ¯: strict_mode={req.strict_mode}, fast_mode={req.fast_mode}")
    
    # 2. æ£€æŸ¥æ˜¯å¦ä¸ºä¸¥æ ¼æ¨¡å¼
    if req.strict_mode:
        # ä¸¥æ ¼æ¨¡å¼ï¼šè·³è¿‡è§„åˆ™åŒ¹é…ï¼Œç›´æ¥ä½¿ç”¨å¤§æ¨¡å‹æ£€æµ‹
        llm_start = time.time()
        llm_result = call_ollama_api(req.text)
        llm_time = time.time() - llm_start
        # å®¹é”™ï¼šè‹¥æ¨¡å‹è¾“å‡ºå¼‚å¸¸ï¼Œé»˜è®¤æŒ‰"æ­£å¸¸"å¤„ç†
        llm_result = llm_result if llm_result in ["æ•æ„Ÿ", "æ­£å¸¸"] else "æ­£å¸¸"
        final_result = llm_result
        
        # è¿”å›ä¸¥æ ¼æ¨¡å¼ç»“æœ
        return {
            "status": "success",
            "data": {
                "original_text": req.text[:100] + "..." if len(req.text) > 100 else req.text,
                "rule_detection": {
                    "ac_results": [],
                    "dfa_results": [],
                    "preprocess_results": [],
                    "all_results": [],
                    "suspicious_segments": [],
                    "word_count": 0,
                    "normalized_text": "",
                    "timing": {"preprocess_time": 0, "ac_time": 0, "dfa_time": 0, "total_time": 0}
                },
                "llm_detected": llm_result,
                "llm_time": round(llm_time * 1000, 2),
                "final_result": final_result,
                "detection_flow": "strict_mode"
            }
        }
    
    # 3. æ™®é€šæ¨¡å¼ï¼šä½¿ç”¨è§„åˆ™åŒ¹é…å¿«é€Ÿç­›é€‰ + å­˜ç–‘å†…å®¹å¤§æ¨¡å‹æ£€æµ‹
    rule_result = three_step_filter.detect(req.text, fast_mode=req.fast_mode)
    
    # 4. åˆ¤æ–­æ˜¯å¦éœ€è¦å¤§æ¨¡å‹æ£€æµ‹ï¼ˆè§„åˆ™åŒ¹é…å¿«é€Ÿç­›é€‰ + å­˜ç–‘å†…å®¹å¤§æ¨¡å‹æ£€æµ‹ï¼‰
    rule_has_sensitive = bool(rule_result['all_results'])  # è§„åˆ™åŒ¹é…æ˜¯å¦å‘ç°æ•æ„Ÿè¯
    
    if rule_has_sensitive:
        # ä»…å¯¹è§„åˆ™åŒ¹é…å‡ºæ•æ„Ÿè¯çš„æ–‡æœ¬è¿›è¡Œå¤§æ¨¡å‹æ£€æµ‹
        llm_start = time.time()
        llm_result = call_ollama_api(req.text)
        llm_time = time.time() - llm_start
        # å®¹é”™ï¼šè‹¥æ¨¡å‹è¾“å‡ºå¼‚å¸¸ï¼Œé»˜è®¤æŒ‰"æ­£å¸¸"å¤„ç†
        llm_result = llm_result if llm_result in ["æ•æ„Ÿ", "æ­£å¸¸"] else "æ­£å¸¸"
        final_result = llm_result  # å¤§æ¨¡å‹æ£€æµ‹ç»“æœå³ä¸ºæœ€ç»ˆç»“æœ
    else:
        # è§„åˆ™åŒ¹é…æ— æ•æ„Ÿè¯ï¼Œç›´æ¥åˆ¤å®šä¸ºæ­£å¸¸
        llm_result = "æ­£å¸¸"
        llm_time = 0
        final_result = "æ­£å¸¸"

    # 4. è¿”å›å“åº”
    return {
        "status": "success",
        "data": {
            "original_text": req.text[:100] + "..." if len(req.text) > 100 else req.text,
            "rule_detection": {
                "ac_results": rule_result['ac_results'],           # ACè‡ªåŠ¨æœºåˆç­›ç»“æœ
                "dfa_results": rule_result['dfa_results'],         # DFAæ£€æµ‹ç»“æœ
                "preprocess_results": rule_result['preprocess_results'],  # é¢„å¤„ç†ç»“æœ
                "all_results": rule_result['all_results'],         # åˆå¹¶åçš„æ‰€æœ‰æ•æ„Ÿè¯
                "suspicious_segments": rule_result['suspicious_segments'],  # å¯ç–‘æ–‡æœ¬ç‰‡æ®µ
                "word_count": rule_result['word_count'],           # è¯åº“ç»Ÿè®¡ä¿¡æ¯
                "normalized_text": rule_result['normalized_text'], # å½’ä¸€åŒ–åçš„æ–‡æœ¬
                "timing": rule_result['timing']                    # è§„åˆ™åŒ¹é…ç”¨æ—¶
            },
            "llm_detected": llm_result,    # å¤§æ¨¡å‹æ£€æµ‹ç»“æœ
            "llm_time": round(llm_time * 1000, 2),  # å¤§æ¨¡å‹æ£€æµ‹ç”¨æ—¶ï¼ˆæ¯«ç§’ï¼‰
            "final_result": final_result,  # æœ€ç»ˆç»“æœ
            "detection_flow": "rule_only" if not rule_has_sensitive else "rule_then_llm"  # æ£€æµ‹æµç¨‹ï¼šè§„åˆ™åŒ¹é…å¿«é€Ÿç­›é€‰ + å­˜ç–‘å†…å®¹å¤§æ¨¡å‹æ£€æµ‹
        }
    }

# ---------------------- æ¨¡å‹ç®¡ç†API ----------------------
@app.get("/model-status", summary="è·å–æ¨¡å‹çŠ¶æ€")
async def get_model_status():
    """è·å–å¤§æ¨¡å‹é¢„çƒ­çŠ¶æ€"""
    current_time = time.time()
    
    status_info = {
        "is_warmed_up": model_warm_up_status["is_warmed_up"],
        "warm_up_time": model_warm_up_status["warm_up_time"],
        "last_call_time": model_warm_up_status["last_call_time"],
        "current_time": current_time
    }
    
    if model_warm_up_status["warm_up_time"]:
        time_since_warmup = current_time - model_warm_up_status["warm_up_time"]
        status_info["time_since_warmup"] = round(time_since_warmup, 2)
        status_info["warmup_status"] = "active" if time_since_warmup < 300 else "stale"
    else:
        status_info["time_since_warmup"] = None
        status_info["warmup_status"] = "not_warmed"
    
    return {
        "status": "success",
        "data": status_info
    }

@app.post("/warm-up-model", summary="é¢„çƒ­Ollamaæ¨¡å‹")
async def warm_up_model_endpoint():
    """é¢„çƒ­Ollamaæ¨¡å‹ï¼Œå‡å°‘ç¬¬ä¸€æ¬¡è°ƒç”¨çš„å»¶è¿Ÿ"""
    try:
        success = warm_up_model()
        if success:
            return {
                "status": "success",
                "message": "æ¨¡å‹é¢„çƒ­æˆåŠŸ"
            }
        else:
            return {
                "status": "error", 
                "message": "æ¨¡å‹é¢„çƒ­å¤±è´¥"
            }
    except Exception as e:
        return {
            "status": "error",
            "message": f"æ¨¡å‹é¢„çƒ­å¼‚å¸¸: {str(e)}"
    }

# ---------------------- æ ¸å¿ƒAPIï¼šæ–‡æ¡£æ£€æµ‹ ----------------------
@app.post("/detect/document", summary="æ–‡æ¡£æ•æ„Ÿè¯æ£€æµ‹ï¼ˆæ”¯æŒtxt/pdf/docx/doc/å›¾ç‰‡OCRï¼Œä¸¥æ ¼æ¨¡å¼ï¼‰")
async def detect_document(file: UploadFile = File(...)):
    # 1. æ ¡éªŒæ–‡ä»¶ç±»å‹ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
    allowed_types = {
        "text/plain": "txt",
        "application/pdf": "pdf",
        "application/vnd.openxmlformats-officedocument.wordprocessingml.document": "docx",
        "application/msword": "doc",  # DOCæ ¼å¼
        "image/jpeg": "jpg", "image/jpg": "jpg", "image/png": "png", 
        "image/bmp": "bmp", "image/gif": "gif", "image/tiff": "tiff"  # å›¾ç‰‡æ ¼å¼ï¼ˆOCRï¼‰
    }
    if file.content_type not in allowed_types:
        raise HTTPException(
            status_code=400,
            detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼æ”¯æŒï¼šTXTã€PDFã€DOCXã€DOCã€å›¾ç‰‡æ ¼å¼ï¼ˆOCRï¼‰"
        )

    # 1.1 æ ¡éªŒæ–‡ä»¶å¤§å°ï¼ˆé™åˆ¶10MBï¼‰
    MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB
    if file.size and file.size > MAX_FILE_SIZE:
        raise HTTPException(
            status_code=400,
            detail=f"æ–‡ä»¶è¿‡å¤§ï¼æ–‡ä»¶å¤§å°ä¸èƒ½è¶…è¿‡10MBï¼Œå½“å‰æ–‡ä»¶å¤§å°ï¼š{file.size / (1024 * 1024):.2f}MB"
        )

    # 2. è¯»å–æ–‡ä»¶å†…å®¹ï¼ˆæ ¹æ®æ–‡ä»¶ç±»å‹è§£æï¼‰
    content = await file.read()
    text = ""
    file_type = allowed_types[file.content_type]
    
    try:
        if file_type == "txt":
            # è§£ætxtï¼ˆé»˜è®¤UTF-8ç¼–ç ï¼Œè‹¥ä¹±ç å¯å°è¯•gbkï¼‰
            text = content.decode("utf-8", errors="ignore")
        elif file_type == "docx":
            # è§£ædocx
            doc = docx.Document(BytesIO(content))
            text = "\n".join([para.text for para in doc.paragraphs])
        elif file_type == "pdf":
            # è§£æpdfï¼ˆå¿½ç•¥æ— æ³•æå–çš„æ–‡æœ¬ï¼‰
            reader = PyPDF2.PdfReader(BytesIO(content))
            text = "\n".join([page.extract_text() for page in reader.pages if page.extract_text()])
        elif file_type == "doc":
            # è§£ædoc - ä½¿ç”¨antiwordå·¥å…·
            try:
                # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
                with tempfile.NamedTemporaryFile(delete=False, suffix='.doc') as temp_file:
                    temp_file.write(content)
                    temp_file_path = temp_file.name
                
                try:
                    # ä½¿ç”¨antiwordå·¥å…·è§£æDOCæ–‡ä»¶
                    result = subprocess.run(
                        ['antiword', temp_file_path],
                        capture_output=True,
                        text=True,
                        timeout=30  # 30ç§’è¶…æ—¶
                    )
                    
                    if result.returncode == 0:
                        text = result.stdout
                        if not text.strip():
                            raise Exception("antiwordæ— æ³•ä»DOCæ–‡ä»¶ä¸­æå–æ–‡æœ¬å†…å®¹")
                    else:
                        raise Exception(f"antiwordæ‰§è¡Œå¤±è´¥ï¼š{result.stderr}")
                        
                finally:
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    try:
                        os.unlink(temp_file_path)
                    except:
                        pass
                        
            except FileNotFoundError:
                raise HTTPException(
                    status_code=500,
                    detail="antiwordå·¥å…·æœªå®‰è£…ã€‚è¯·é‡æ–°æ„å»ºDockeré•œåƒä»¥ç¡®ä¿antiwordå·¥å…·å·²æ­£ç¡®å®‰è£…ã€‚"
                )
            except subprocess.TimeoutExpired:
                raise HTTPException(
                    status_code=500,
                    detail="DOCæ–‡ä»¶è§£æè¶…æ—¶ï¼Œæ–‡ä»¶å¯èƒ½è¿‡å¤§æˆ–æ ¼å¼å¼‚å¸¸ã€‚"
                )
            except Exception as doc_error:
                raise HTTPException(
                    status_code=400,
                    detail=f"DOCæ–‡ä»¶è§£æå¤±è´¥ï¼š{str(doc_error)}ã€‚å»ºè®®å°†DOCæ–‡ä»¶è½¬æ¢ä¸ºDOCXæ ¼å¼åé‡æ–°ä¸Šä¼ ã€‚"
                )
        elif file_type in ["jpg", "png", "bmp", "gif", "tiff"]:
            # OCRæ–‡å­—è¯†åˆ« - ä½¿ç”¨pytesseract + Tesseract OCRå¼•æ“
            try:
                # æ‰“å¼€å›¾ç‰‡
                image = Image.open(BytesIO(content))
                
                # å›¾ç‰‡é¢„å¤„ç†ä»¥æé«˜OCRè¯†åˆ«ç‡
                processed_image = preprocess_image_for_ocr(image)
                
                # è·å–OCRé…ç½®
                ocr_config = get_ocr_config()
                
                # æ„å»ºOCRé…ç½®å­—ç¬¦ä¸²
                if ocr_config['char_whitelist']:
                    full_config = f"{ocr_config['config']} -c tessedit_char_whitelist={ocr_config['char_whitelist']}"
                else:
                    full_config = ocr_config['config']
                
                # æ‰§è¡ŒOCRè¯†åˆ«
                text = pytesseract.image_to_string(
                    processed_image, 
                    lang=ocr_config['lang'], 
                    config=full_config
                )
                
                # æ¸…ç†è¯†åˆ«ç»“æœ
                text = text.strip()
                
                # æ·»åŠ OCRè¯†åˆ«ç»“æœè°ƒè¯•ä¿¡æ¯
                print(f"OCRè¯†åˆ«ç»“æœ: '{text}'")
                print(f"OCRè¯†åˆ«ç»“æœé•¿åº¦: {len(text)}")
                
                # å¦‚æœè¯†åˆ«ç»“æœä¸ºç©ºï¼Œå°è¯•ä½¿ç”¨æ›´å®½æ¾çš„é…ç½®
                if not text.strip():
                    print("ä½¿ç”¨å®½æ¾é…ç½®é‡æ–°å°è¯•OCRè¯†åˆ«...")
                    text = pytesseract.image_to_string(
                        processed_image, 
                        lang=ocr_config['lang'], 
                        config='--psm 3 --oem 3'  # æ›´å®½æ¾çš„é…ç½®
                    ).strip()
                    print(f"å®½æ¾é…ç½®OCRè¯†åˆ«ç»“æœ: '{text}'")
                
            except Exception as ocr_error:
                raise HTTPException(
                    status_code=500, 
                    detail=f"OCRè¯†åˆ«å¤±è´¥ï¼š{str(ocr_error)}ã€‚è¯·ç¡®ä¿å›¾ç‰‡æ¸…æ™°ä¸”åŒ…å«å¯è¯†åˆ«çš„æ–‡å­—å†…å®¹ã€‚"
                )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ–‡æ¡£è§£æå¤±è´¥ï¼š{str(e)}")

    # 3. æ ¡éªŒè§£æç»“æœï¼ˆæ–‡æ¡£å†…å®¹ä¸èƒ½ä¸ºç©ºï¼‰
    if not text.strip():
        raise HTTPException(status_code=400, detail="æ–‡æ¡£å†…å®¹ä¸ºç©ºæˆ–æ— æ³•æå–æ–‡æœ¬")

    # 4. æ–‡æ¡£æ£€æµ‹ï¼šæ–‡æœ¬é¢„å¤„ç† + ä¸¥æ ¼æ¨¡å¼ï¼ˆç›´æ¥ä½¿ç”¨å¤§æ¨¡å‹æ£€æµ‹ï¼‰
    # 4.1 æ–‡æœ¬é¢„å¤„ç†ï¼ˆå½’ä¸€åŒ–å­—ç¬¦æ ¼å¼ï¼‰
    preprocessor = TextPreprocessor()
    normalized_text = preprocessor.preprocess_text(text)
    
    # 4.2 ä½¿ç”¨é¢„å¤„ç†åçš„æ–‡æœ¬è¿›è¡ŒLLMæ£€æµ‹
    llm_start = time.time()
    llm_result = call_ollama_api(normalized_text)
    llm_time = time.time() - llm_start
    # å®¹é”™ï¼šè‹¥æ¨¡å‹è¾“å‡ºå¼‚å¸¸ï¼Œé»˜è®¤æŒ‰"æ­£å¸¸"å¤„ç†
    llm_result = llm_result if llm_result in ["æ•æ„Ÿ", "æ­£å¸¸"] else "æ­£å¸¸"
    final_result = llm_result

    # 5. è¿”å›å“åº”
    return {
        "status": "success",
        "data": {
            "filename": file.filename,
            "file_type": file_type,
            "text_length": len(text),  # æå–çš„æ–‡æœ¬é•¿åº¦
            "rule_detection": {
                "ac_results": [],
                "dfa_results": [],
                "preprocess_results": [],
                "all_results": [],
                "suspicious_segments": [],
                "word_count": 0,
                "normalized_text": normalized_text,  # å½’ä¸€åŒ–åçš„æ–‡æœ¬
                "timing": {"preprocess_time": 0, "ac_time": 0, "dfa_time": 0, "total_time": 0}
            },
            "llm_detected": llm_result,
            "llm_time": round(llm_time * 1000, 2),  # å¤§æ¨¡å‹æ£€æµ‹ç”¨æ—¶ï¼ˆæ¯«ç§’ï¼‰
            "final_result": final_result,
            "detection_flow": "strict_mode"  # æ–‡æ¡£æ£€æµ‹ä½¿ç”¨ä¸¥æ ¼æ¨¡å¼ï¼ˆé¢„å¤„ç†+LLMï¼‰
        }
    }

# ---------------------- å¥åº·æ£€æŸ¥ç«¯ç‚¹ ----------------------
@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {
        "status": "healthy",
        "timestamp": time.time(),
        "version": "1.0.0"
    }

# ---------------------- å¯åŠ¨å…¥å£ï¼ˆä¾›å®¹å™¨å†…æ‰§è¡Œï¼‰ ----------------------
if __name__ == "__main__":
    import uvicorn
    # å¯åŠ¨UVicornæœåŠ¡ï¼ˆhost=0.0.0.0ï¼šå…è®¸å®¹å™¨å¤–éƒ¨è®¿é—®ï¼‰
    uvicorn.run(
        app="main:app",
        host="0.0.0.0",
        port=8000,
        workers=1  # å•è¿›ç¨‹ï¼ˆå¤§æ¨¡å‹å ç”¨å†…å­˜é«˜ï¼Œå¤šè¿›ç¨‹æ˜“OOMï¼‰
    )
